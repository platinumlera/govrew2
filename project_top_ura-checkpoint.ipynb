{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/Users/polinailenkova/Downloads/dataset_clear.csv' does not exist: b'/Users/polinailenkova/Downloads/dataset_clear.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-274d983466b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# in_data = csv.reader(open(INPUT_FILE, encoding='utf8', newline=''), delimiter=';')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0min_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINPUT_FILE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# this gives us the size of the array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/Users/polinailenkova/Downloads/dataset_clear.csv' does not exist: b'/Users/polinailenkova/Downloads/dataset_clear.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "INPUT_FILE = '/Users/polinailenkova/Downloads/dataset_clear.csv' \n",
    "TEXT_FIELD = 0 \n",
    "\n",
    "# in_data = csv.reader(open(INPUT_FILE, encoding='utf8', newline=''), delimiter=';') \n",
    "\n",
    "in_data = pd.read_csv(INPUT_FILE, delimiter=';',decimal='.', encoding='utf8')\n",
    "\n",
    "# this gives us the size of the array\n",
    "print('Dataset size', in_data.shape)\n",
    "\n",
    "# here we can get size of array as two variables\n",
    "num_rows, num_feature = in_data.shape\n",
    "\n",
    "print('row number: ', num_rows)\n",
    "print('feature number: ', num_feature)\n",
    "print('names of features: ', list(in_data))\n",
    "\n",
    "# ваши оценки лежат в колонке 'Eval', поэтому можно просто присвоить колонку вектору\n",
    "# Создаем вектор ответов\n",
    "Y = in_data['Eval'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pymystem3 \n",
    "import regex\n",
    "\n",
    "empty = 0 \n",
    "\n",
    "# тут будет хранится список оригинальных текстов \n",
    "x_data = [] \n",
    "# тут будет хранится список лематизированных текстов \n",
    "y_data = [] \n",
    "\n",
    "\n",
    "# создание объекта mystem \n",
    "m = pymystem3.Mystem() \n",
    "# для простоты укажу 100 документов, если нужно полный набор отлематизировать, \n",
    "# то нужно поставить число документов, ну или num_rows\n",
    "for i in range(num_rows):\n",
    "    text = str(in_data['Message'][i]) \n",
    "    s = text.replace('\"', '') \n",
    "    s = m.lemmatize(s) \n",
    "    s = ''.join(s) \n",
    "    lemans=str(s) \n",
    "    lemans = regex.sub('[!@#$1234567890#]', '', lemans) \n",
    "\n",
    "    #print(''.join(lemmas))\n",
    "    print(i)\n",
    "    x_data.append(text) \n",
    "    y_data.append(lemans) \n",
    " \n",
    "print('Загрузка данных и процедура лематизации завершена')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь проведем векторизацию\n",
    "# проводим векторизацию и удаление стоп слов\n",
    "# we define the file name with a list of stop words\n",
    "file_name ='/Users/polinailenkova/Downloads/my_stop.txt'\n",
    "# Please note that we upload a list of stop words in the data frame\n",
    "words = pd.read_csv(file_name, encoding='utf8')\n",
    "# now wa can convert dataframe into list\n",
    "#print('names of features: ', list(words))\n",
    "\n",
    "my_stop = words['Stop_words'].values.tolist()\n",
    "# we define list of word\n",
    "# print('the list of stop words: ', my_stop)\n",
    "\n",
    "#In this code, we create a procedure for transforming documents into a matrix based on a list of stop-stop words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(ngram_range=(1,1),stop_words=(my_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем процедуру векторизации\n",
    "x = vec.fit_transform(y_data)\n",
    "# transform doc to matrix\n",
    "data = vec.fit_transform(y_data).toarray()\n",
    "print('процесс векторизации закончен')\n",
    "\n",
    "num_docs, num_feature =x.shape\n",
    "print('doc number: ', num_docs, 'feature number: ', num_feature)\n",
    "\n",
    "# print(y_data)\n",
    "myfich = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "# число фич\n",
    "k=1000\n",
    "my_test = SelectKBest(chi2, k)\n",
    "my_fit = my_test.fit(x, Y)\n",
    "\n",
    "# определяем количество значимых цифр\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "print('Fit score for all data: ', my_fit.scores_)\n",
    "\n",
    "plt.plot(my_fit.scores_,'-bo')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X_new = my_fit.transform(x)\n",
    "new_list = my_fit.get_support(indices=True)\n",
    "print('Размер массива нового массива', X_new.shape)\n",
    "print()\n",
    "print('Список оптимальных фич: ', new_list)\n",
    "\n",
    "print(new_list)\n",
    "\n",
    "print()\n",
    "# список фич и scores\n",
    "for i in range(len(new_list)-1):\n",
    "    j = new_list[i]\n",
    "    print(myfich[j],' - ', my_fit.scores_[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фичи лежат в data\n",
    "# ответы в YY\n",
    "# Проведем разделение текстов на две части\n",
    "# импортируем поцедуру деления датетса на тестовую и обучающую коллекцию.\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Делим нашу клддекцию на две части: 1. часть для обучения 2. часть для проверки результатов обучения\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.33, random_state=42)\n",
    "print('Test collection size: ', X_test.shape)\n",
    "print('Training collection size: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модели: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "## Import the Classifier.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# устанавливаем число соседей\n",
    "n_neighbors=1\n",
    "\n",
    "# тип взвешивания: ‘uniform’ : uniform weights, ‘distance’ : weight points by the inverse of their distance\n",
    "weights = 'uniform'\n",
    "\n",
    "# тип расстояния (p = 1 - manhattan_distance, p = 2 - euclidean_distance, )\n",
    "p = 2\n",
    "\n",
    "# автоматический подбор типа алгоритма\n",
    "algorithm = 'auto'\n",
    "\n",
    "#Создаем модель\n",
    "knn = KNeighborsClassifier(n_neighbors, weights, algorithm, p)\n",
    "\n",
    "## Обучаем модель на тренироваочных данных\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# оцениваем точность нашей модели на основе известных значений y_train\n",
    "predicted = knn.predict(X_test)\n",
    "\n",
    "# расчитываем основные метрики качества\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "print('confusion matrix')\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "neighbors=10\n",
    "weights = 'uniform'\n",
    "p = 2\n",
    "#организуем пустой массив scores\n",
    "cv_scores_train = []\n",
    "cv_scores_test = []\n",
    "\n",
    "\n",
    "# в цикле перебираем число соседей и расчитываем среднее значение F1\n",
    "for k in range(1, neighbors):\n",
    "    knn = KNeighborsClassifier(k, weights, algorithm, p)\n",
    "    knn.fit(X_train, y_train)\n",
    "    # knn.score(X_test, y_test)\n",
    "    # подсчет среднего значения F1 при разных разбиениях\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "    # расчет среднего значения scores по всем разбиениям\n",
    "    #print(np.mean(scores))\n",
    "    cv_scores_train.append(np.mean(scores))\n",
    "    \n",
    "    scores = cross_val_score(knn, X_test, y_test, cv=5, scoring='f1_macro')\n",
    "    # добавляем в список очереное среднее значение f1_macro\n",
    "    cv_scores_test.append(np.mean(scores))\n",
    "   \n",
    "    # вывод среднего значения F1 по всем классам\n",
    "    #print(f1_score(y_test, knn.predict(X_test), average='macro'))\n",
    "    # расчитываем F1 и добавляем его в список\n",
    "    #cv_scores_test.append(f1_score(y_test, knn.predict(X_test), average='macro'))\n",
    "\n",
    "# строим график F1\n",
    "plt.plot(cv_scores_train, 'ro', alpha=0.2)\n",
    "plt.plot(cv_scores_train, '-bo', linewidth=1)\n",
    "\n",
    "plt.plot(cv_scores_test, 'ro', alpha=0.2)\n",
    "plt.plot(cv_scores_test, 'r', alpha=0.2)\n",
    "plt.show()    \n",
    "print('расчет закончен')\n",
    "#1 сосед топ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors=4\n",
    "\n",
    "# тип взвешивания: ‘uniform’ : uniform weights, ‘distance’ : weight points by the inverse of their distance\n",
    "weights = 'uniform'\n",
    "\n",
    "# тип расстояния (p = 1 - manhattan_distance, p = 2 - euclidean_distance, )\n",
    "p = 2\n",
    "\n",
    "# автоматический подбор типа алгоритма\n",
    "algorithm = 'auto'\n",
    "\n",
    "#Создаем модель\n",
    "knn = KNeighborsClassifier(n_neighbors, weights, algorithm, p)\n",
    "\n",
    "## Обучаем модель на тренироваочных данных\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# оцениваем точность нашей модели на основе известных значений y_train\n",
    "predicted = knn.predict(X_test)\n",
    "\n",
    "# расчитываем основные метрики качества\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "\n",
    "print('confusion matrix')\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN не оч"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем модель\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "# определяем модель классификатора\n",
    "model = SVC()\n",
    "# обучаем модель на тренировочном датасете\n",
    "model.fit(X_train, y_train)\n",
    "# выводим на экран параметры модели.\n",
    "print(model)\n",
    "\n",
    "# строим предсказание\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# определяем качество модели\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "# строим confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1, 10], 'kernel': ['linear']},\n",
    "  {'C': [1, 10], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "svc = svm.SVC(gamma=\"scale\")\n",
    "clf = GridSearchCV(svc, param_grid, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "# выводим наилучший результат\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем модель классификатора\n",
    "model = SVC(C=10, kernel='linear')\n",
    "# обучаем модель на тренировочном датасете\n",
    "model.fit(X_train, y_train)\n",
    "# выводим на экран параметры модели.\n",
    "print(model)\n",
    "\n",
    "# строим предсказание\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# определяем качество модели\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "# строим confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC ниче так 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    criterion='gini',\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight='balanced')\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "# строим предсказание\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "# определяем качество модели\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "# строим confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    criterion='gini',\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight='balanced')\n",
    "\n",
    "clf = GridSearchCV(estimator= clf, param_grid=param_grid, cv= 5, scoring='f1_macro')\n",
    "clf.fit(X_train, y_train)\n",
    "# выводим наилучший результат\n",
    "print('best parameters: ', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    criterion='entropy',\n",
    "    max_depth=8,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features='auto',\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_impurity_split=None,\n",
    "    bootstrap=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight='balanced')\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "# строим предсказание\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "# определяем качество модели\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "# строим confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "НЕПЛОХО 52\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# и так, мы обучили классификатор KNN (для простоты не делал поиск оптимального числа соседей)\n",
    "#  теперь читаем новые данные, по которым нужно сделать предсказание\n",
    " #читаем новые данные \n",
    "f = '/Users/polinailenkova/Desktop/data_new.csv'\n",
    "df1=pd.read_csv(f,  sep=';', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this gives us the size of the array\n",
    "print('Dataset size', df1.shape)\n",
    "\n",
    "# here we can get size of array as two variables\n",
    "num_rows1, num_feature1 = df1.shape\n",
    "\n",
    "print('row number: ', num_rows1)\n",
    "print('feature number: ', num_feature1)\n",
    "print('names of features: ', list(df1))\n",
    "\n",
    "print('-------------------')\n",
    "print('full data loaded')\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для простоты возьмем 200 текстов\n",
    "# проводим лематизацию этого датасета\n",
    "y_data1 = []\n",
    "# how we arrange cycle for lematization of 200 documents\n",
    "for i in range(500):\n",
    "    # getting doc\n",
    "    text = str(df1['Text'][i])\n",
    "    # remove quotes from text \n",
    "    s = text.replace('\"', '')\n",
    "    # how we are doing lematization\n",
    "    s = m.lemmatize(s)\n",
    "    s = ''.join(s)\n",
    "    lemans=str(s)\n",
    "    lemans = regex.sub('[!@#$1234567890#—ツ►๑۩۞۩•*”˜˜”*°°*`]', '', lemans)\n",
    "    y_data1.append(lemans)    \n",
    "    print('doc number : ', i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь проводим векторизацию используя уже обученный векторизатор\n",
    "count_vector=vec.transform(y_data1)\n",
    "\n",
    "# transform doc to matrix\n",
    "data1 = vec.transform(y_data1).toarray()\n",
    "print('процесс векторизации закончен')\n",
    "print('размер новой матрицы: ', data1.shape)\n",
    "data_new = my_fit.transform(data1)\n",
    "print(data_new.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь можно сделать предсказание\n",
    "# вот теперь подставить полученную матрицу data1 в уже обученный классификатор\n",
    "y_predicted1 = model.predict(data_new)\n",
    "# предсказание сентимента новых текстов.\n",
    "print(y_predicted1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
